% !TEX root = ../thesis.tex
\chapter{Goal of the~thesis}\label{sec:goal}
In this thesis we are focus on improving linear prediction in sales and financial forecasting and combine it with
modern machine learning approaches. Machine learning techniques are used to improve the~accurate of linear
prediction models. Specifically, there are two ways in which machine learning are applied to linear prediction:
\begin{enumerate}
    \item Feature engineering: In this approach, machine learning is used to extract relevant features from the~input
    signal that can be used as input to a~linear prediction model.The extracted features can capture complex patterns
    in the~data that are not captured by the~raw input. Feature engineering can be done using techniques such as
    principal component analysis, wavelet transform, and Fourier transform.
    \item Model selection and training: In this approach, machine learning is used to select the~best linear
    model for the~prediction task and to estimate its parameters from the~data. This can involve selecting the~best
    set of input variables for the~linear model, choosing the~best regularization parameter to avoid overfitting, and
    optimizing the~model hyperparameters. Common machine learning algorithms used for linear prediction include linear
    regression, support vector regression, and artificial neural networks.
\end{enumerate}
The goal of long-term linear prediction is to estimate future values of a~signal or time series based on its past
values using a~linear model.
The linear prediction model uses a~set of coefficients to weight past values of the~signal and produce a~prediction for
future values. the~accurate of the~prediction depends on the~quality of the~model and the~complexity of the
underlying signal. Combine this principles get to us the~best results from linear prediction over
sales and financial datasets.
\chapter{Methodology}\label{sec:methodology}
    \section{Characteristics of the~research object}\label{subsec:research_object}
    Sales data and financial datasets are two broad categories of data that have different characteristics and are used
    for different purposes. Here are some general characteristics of each type of dataset:\\
    \\
    \textbf{Sales data}\\
    Typically contains transactional information, such as the~date, time, location, and amount of a~purchase. Can include additional information about the~customer, such as their demographic profile, purchase history, and
    preferences. Often analyzed to understand customer behavior, such as buying patterns, trends, and preferences
    May have seasonal or cyclical patterns, depending on the~nature of the~product or service being sold
    Can be used to optimize marketing and sales strategies, such as targeting specific customer segments, promoting
    certain products, or adjusting prices and discounts.\\
    \\
    \textbf{Financial datasets}\\
    Typically contains financial information, such as the~revenue, expenses, assets, liabilities, and cash
    flow of a~company or organization can include additional information about the~market, such as interest rates,
    exchange rates, and stock prices. Often analyzed to evaluate the~financial performance and health of a~company,
    such as profitability, solvency, and liquidity. May have regulatory or compliance requirements, such as financial
    reporting standards or tax laws. Can be used to make strategic decisions, such as investment, merger and
    acquisition, or divestiture. Both sales data and financial datasets can be used for forecasting and modeling,
    but they have different analytical techniques and tools. Sales data often requires customer segmentation,
    predictive analytics, and machine learning algorithms, while financial datasets require financial ratio analysis,
    time series forecasting, and risk assessment.
    \section{Methods}\label{subsec:methods}
    In our thesis was used mainly these list of methods like a linear prediction, regression, backpropagation or statistical  measures.
        \subsection{Linear regression}\label{sec:linear}
        Linear regression~\cite{Levinson} attempts to model the~relationship between two variables by fitting a~linear
        equation to observed data. One variable is considered to be~an~explanatory variable, and the~other is
        considered to be a~dependent variable. For example, we want to relate the~weights of individuals to their
        heights using a~linear regression model. Before attempting to fit a~linear model to observed data, we should
        first determine if there exists a~relationship between the~variables of interest. This does not necessarily
        mean that one variable causes the~other, but that there is some significant association between them.
        To determine the~strength of relationship a~scatterplot can be a~helpful tool.
        If there appears to be no association between the~proposed explanatory and dependent variables, then
        fitting a~linear regression model to the~data probably will not provide a~useful model.
        a~valuable numerical measure of association between two variables is the~correlation coefficient.
        That is a~value from $[-1, 1]$ range indicating the~strength of the~association of the~observed data
        for the~two variables.\\
        a~linear regression line has~an~equation of the~form $Y = a~+ bX$, where $X$ is the~explanatory variable
        and $Y$ is the~dependent variable. the~slope of the~line is $b$, and $a$ is the~intercept.
        Example of some basic linear models:\\
        \begin{equation} \label{eq:4}
        \begin{array}{l@{}l}
            Y = ax + b\\
            Y = a~+ bx + c\\
            Y = a\sin x + b\\
        \end{array}
        \end{equation}
        \subsection{Linear prediction}\label{subsec:lp}
        Linear prediction is a~statistical technique used to forecast future values based on past observations.
        It is a~method for modeling the~relationship between a~dependent variable and one or more independent
        variables in a~linear form. the~goal of linear prediction is to find the~best linear approximation of the
        relationship between the~variables, which can then be used to make predictions about future
        values of the~dependent variable.
        \subsection{Backpropagation}\label{subsec:backpropagation}
        Backpropagation is a~widely used algorithm for computing the~gradients of the~loss function with
        respect to the~weights of a~neural network. It is the~backbone of training deep neural networks using
        stochastic gradient descent (SGD) or its variants.
        the~backpropagation algorithm computes the~gradient of the~loss function with respect to each
        weight in the~network by recursively applying the~chain rule of differentiation.
        the~algorithm is typically implemented in two phases:
        \begin{enumerate}
            \item Forward pass: the~forward pass involves computing the~output of each layer in the~network,
            starting from the~input layer and propagating through the~hidden layers to the~output layer.
            the~output of each layer is computed as a~function of the~input to the~layer and the~weights of the~layer.
            the~forward pass computes the~predictions of the~network on a~given input.
            \item Backward pass: the~backward pass involves computing the~gradients of the~loss function with
            respect to each weight in the~network, starting from the~output layer and propagating backwards
            through the~hidden layers to the~input layer. the~gradients are computed by applying the~chain rule of
            differentiation to the~output of each layer. the~gradients are then used to update the~weights of
            the~network using~an~optimization algorithm such as SGD.
        \end{enumerate}
        the~backpropagation algorithm can be optimized using various techniques, such as parallel computing, weight
        sharing, and regularization. It is a~powerful tool for training deep neural networks with many layers and
        millions of parameters, and has enabled significant advances in many areas of machine learning, including
        computer vision, natural language processing, and speech recognition.
    \section{Datasets}\label{subsec:datasets}
        Creating a~science research dataset involves several steps, which may vary depending on the~nature of the
        research and the~specific field of science. Here are some general steps to consider:
        \begin{enumerate}
            \item Determine the~research question: the~first step in creating a~research dataset is to determine the
            research question. This will help we identify the~data we need to collect and the~type of dataset
            we need to create.
            \item Choose the~data sources: Once we have identified the~research question, we need to determine
            the~data sources we will use to create our dataset. Depending on the~research question, we may use
            publicly available data, data from surveys or experiments, data from literature, or a~combination of these.
            \item Collect and organize the~data: Collecting data can involve different methods, such as surveys,
            experiments, literature reviews, or data mining. We need to make sure that the~data we collect is reliable,
            accurate, and relevant to we research question. Once you have collected the~data, we need to organize
            it in a~way that is easy to analyze.
            \item Clean and preprocess the~data: Before we can analyze the~data, we need to clean and preprocess it.
            This involves removing any errors, missing values, or duplicates in the~data. We may also need to
            transform or normalize the~data to make it compatible with the~analysis methods we plan to use.
            \item Perform exploratory analysis: Once the~data is cleaned and preprocessed, we can perform exploratory
            analysis to identify patterns, trends, or relationships in the~data. This can help us refine our research
            question or identify areas that require further investigation.
            \item Perform statistical analysis: Depending on the~research question, we may need to perform statistical
            analysis to test hypotheses or evaluate the~significance of the~results. This can involve using regression
            analysis, hypothesis testing, or other statistical methods.
        \end{enumerate}
        Validate the~dataset: After we have analyzed the~data, we need to validate the~dataset to ensure that the
        results are accurate and reliable. This can involve comparing the~results to other datasets or conducting
        further experiments to verify the~results. Overall, creating a~science research dataset involves careful
        planning, data collection, preprocessing, and analysis to ensure that the~data is accurate, reliable, and
        relevant to the~research question.
    \section{Comparison criteria}\label{subsec:comparison}
    Finally we define the~method to compare our models results.
    Absolute number of income value prediction should not be important for the~store owners because of that we
    calculated the~aberration for each month prediction and then we easily calculate quarterly and yearly results.
    the~sum of squared errors (SSE), defined by:
    $$SSE = \sum^n_{i=1}w_i(y_i - \overline{y_i})^2,$$
    between the~fitting models and the~used data serves as the~fitting criterion,
    with values closer to $0$ indicating a~smaller random error component of the~model.
    Also some other quality measures were evaluated, \textit{i.e.} the~R-square from interval $[0,\ 1]$,
    that indicates the~proportion of variance satisfactory explained by the~fitting-model (\textit{e.g.}
    R-square $= 0.7325$ means that the~fit explains $73.25\%$ of the~total variation in the~data about the~average);
    R-square is defined as the~ratio of the~sum of squares of the~regression (SSR) and the~total sum of squares (SST).
    SSR is defined as
    $$SSR = \sum_{i=1}^nw_i(\overline{y_i} - \overline{y_i})^2.$$
    SST is also called the~sum of squares about the~mean, and is defined as
    $$SST = \sum_{i=1}^nw_i(y_i - \overline{y})^2,$$
    where SST = SSR + SSE. Givenm these definition, R-square is expressed as
    $$\frac{SSR}{SST} = 1 - \frac{SSE}{SST}.$$
    The~adjusted R-square statistic, with values smaller or equal to $1$, where values closer to $1$ indicate a~better
    fit; the~root mean squared error (RMSE):\\
    $$RMSE = s = \sqrt{\frac{SSE}{v}}$$
    with values closer to $0$ indicating a~fit more useful for prediction~\cite{Jandera2021}.\\
    \\
    Mean square error (MSE) is a~commonly used metric to measure the~average squared difference between the~actual and
    predicted values of a~continuous variable. It is a~measure of how well a~prediction model performs in predicting
    continuous outcomes.\\
    To calculate the~MSE, we first take the~difference between the~predicted and actual values for each data point,
    square these differences, and then take the~average of the~squared differences across all data points.\\
    \\
   The~formula for calculating MSE is:

    \begin{equation}
    MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2,
    \end{equation}
    \\
    where n is the~total number of data points, $y_i$ is the~actual value of the~$i-th$ data point, and $\hat{y}_i$ is
    the~predicted value of the~$i-th$ data point.\\
    \\
    the~square of the~difference in each data point ensures that the~errors are positive and provides a~way to measure
    the~magnitude of the~error. By squaring the~errors, the~MSE gives more weight to larger errors and less weight to
    smaller errors.\\
    \\
    MSE is a~popular metric used in many fields, such as machine learning, statistics, and engineering, to evaluate the
    performance of prediction models. a~lower MSE indicates better performance of the~model in predicting the~outcomes.
    \section{Statistics measures} \label{subsec:statistics}
    \textbf{Median}\\
   ~The~median is a~statistical measure that represents the~middle value of a~dataset. It is a~value that separates
    the~dataset into two halves: half of the~values are greater than the~median, and half of the~values are less than
    the~median. In other words, the~median is the~value that is exactly in the~middle of the~dataset when the
    values are arranged in order of magnitude.
    \\
    To compute the~median of a~dataset, we first sort the~values in ascending or descending order. If the~dataset
    has~an~odd number of values, the~median is the~middle value. For example, in the~dataset ${1, 2, 3, 4, 5}$, the
    median is $3$. If the~dataset has~an~even number of values, the~median is the~average of the~two middle values.
    For example, in the~dataset ${1, 2, 3, 4}$, the~median is $(2 + 3) / 2 = 2.5$.
    \\
   ~The~median is a~robust statistic, meaning that it is not affected by outliers or extreme values in the~dataset,
    unlike the~mean. This makes it a~useful measure of central tendency in datasets with a~large number of outliers or
    skewed distributions. the~median is commonly used in various applications, such as finance, economics, and social
    sciences, to summarize and compare datasets.\\
    \\
    \textbf{Standard deviation}\\
    Standard deviation is a~statistical measure that quantifies the~amount of variation or dispersion in a~dataset.
    It measures how far the~values in a~dataset deviate from the~mean, or average, of the~dataset. the~standard
    deviation is a~non-negative number and has the~same units as the~data being measured.
    \\
    To compute the~standard deviation of a~dataset, we first calculate the~mean of the~dataset.
    Then, we calculate the~difference between each value in the~dataset and the~mean, square each difference, and
    sum up the~squared differences. Finally, we divide the~sum of squared differences by the~number of values in the
    dataset, and take the~square root of the~result. This gives us the~standard deviation of the~dataset.
    \\
    a~small standard deviation indicates that the~values in the~dataset are tightly clustered around the~mean,
    while a~large standard deviation indicates that the~values are widely spread out from the~mean. Standard
    deviation is commonly used in various applications, such as finance, engineering, and natural sciences, to
    analyze and compare datasets. It is also~an~important parameter in many statistical tests and models, such as the
    normal distribution and the~t-test.

\chapter{Syntactic part}\label{sec:syntactic}
Based on the~chapter~\ref{sec:analytical} let us create new mathematical models and approaches to made a~fast
and accurate sales forecasting consists of long-therm linear prediction with individual weights calculated for each
period all based on Levinson-Durbin scheme caled Extended linear prediction (ELP).
We expect to get better results than by using prediction based on short-term or standard long-term linear
prediction (see section~\ref{sec:lp}).  Finally, our approach will return future number of orders for sales companies
based on previous data with better aberration than linear prediction has.
    \section{Predictor Order estimation}\label{sec:ordercalc}
        This section describe our created Neural Network which we used to detect optimal order of predictor (long-term and short-term) for our dataset to get the best results of final forecasting.\\
        \\
        \textbf{Create neural network to estimate predictor order}\\
        \\
        Creating a~neural network for linear prediction and optimal order detection involves several steps.
        Here's a~general overview of the~process:
        \begin{enumerate}
            \item Data Preparation: Collect a~dataset of input/output pairs that represent the~relationship we want
            the~neural network to learn. For linear prediction, this could be a~time-series dataset where we want to
            predict the~next value based on the~previous values. For optimal order detection, this could be a~dataset
            where we have inputs and the~corresponding optimal order.
            \item Data Preprocessing: Preprocess the~dataset by normalizing the~inputs and outputs to a~common range,
            shuffling the~dataset, and splitting it into training, validation, and testing sets.
            \item Model Architecture: Choose~an~appropriate neural network architecture for our problem. For linear
            prediction, we could use a~recurrent neural network (RNN) such as LSTM or GRU. For optimal order
            detection, we could use a~feedforward neural network.
            \item Training: Train the~neural network on the~training set using~an~appropriate optimization algorithm
            such as stochastic gradient descent (SGD), Adam, or RMSProp. During training, monitor the~performance of the
            network on the~validation set to detect overfitting and adjust the~hyperparameters accordingly.
            \item Testing: Evaluate the~performance of the~trained neural network on the~testing set
            to get~an~estimate of its generalization ability.
            \item Deployment: Deploy the~trained neural network to make predictions on new data.
        \end{enumerate}
        \textbf{Choose activation function}\\
        \\
        Choosing the~right activation function for linear prediction order detection in neural networks depends on
        several factors such as the~type of data, the~complexity of the~problem, and the~architecture of the~neural
        network. In our approach we test some most common used activation functions to test and choose the right one.
        Linear activation function was not return good results because of our problem is not linear. ReLU activation function is one of the~most popular activation functions used in deep learning. Sigmoid activation function
        maps any input value to a~value between 0 and 1 so for our problem not good because of we are not solving binnary problems. In general, it's a~good idea to start with the~ReLU activation function and see if it works well for our problem.\\
        \\  
        \textbf{Train neural network}\\
        \\
        To training our neural network we used training dataset created from our main dataset as half a year order data. Then we feed our neural network by that dataset to adjusting the weights of its connections between neurons to minimize the difference between the network's output and the correct output for each input. This process is known as backpropagation.\\
        \\
        The training process is typically repeated many times, with the network being presented with different sets of data each time, until it reaches a satisfactory level of accuracy on a validation set of data. Once the network is trained, it can be used to make predictions or decisions on new, unseen data.\\
        \\
        \textbf{Run neural network to get optimal order}\\
        Our trained Neural network is consist of 4 layers with 40 neurons in input layer than two hidden layers with 20 and 10 neurons and 5 neuron in output layer. Our trained network use ReLU activation function. Training was made over 120 of observations.\\
        \\
        Once a~neural network is trained, there are several steps we can take to get the~best performance out of it:
        \begin{enumerate}
            \item Use the~validation set to tune hyperparameters: the~validation set is a~dataset that is separate
            from the~training and testing sets and is used to tune hyperparameters such as learning rate, batch size,
            and number of epochs. By experimenting with different hyperparameters, we can find the~optimal settings
            that result in the~best performance.
            \item Test the~model on a~separate dataset: Once the~model is trained and the~hyperparameters are tuned,
            it's important to test the~model on a~separate dataset to evaluate its performance. This dataset should be
            completely separate from the~training and validation sets.
            \item Use data augmentation techniques: Data augmentation techniques such as rotation, translation, and
            scaling can be used to generate additional training data, which can improve the~performance of the~model.
            \item Use ensembling techniques: Ensembling techniques such as bagging and boosting can be used to
            combine the~predictions of multiple models to improve the~overall performance.
            \item Use regularization techniques: Regularization techniques such as L1 and L2 regularization
            can be used to prevent overfitting and improve the~generalization of the~model.
            \item Fine-tune the~model on new data: If new data becomes available, the~model can be fine-tuned on the
            new data to further improve its performance.
            \item By following these steps, we can get the~best performance out of a~trained neural network.
            It's important to remember that there is no single approach that works best for all problems, and
            experimentation is often necessary to find the~optimal solution.
        \end{enumerate}
    \section{Shift estimation}\label{sec:shiftcalc}
    I nthis section we see second Neural Network which replace the basic Autocorrelation approach.This neural network will be used for long-term linear prediction.
        \subsection{Autocorrelation approach} \label{subsec:acorr}
        \begin{enumerate}
            \item Compute the~autocorrelation function of the~signal using the~xcorr function in MATLAB.
           The~xcorr function returns the~cross-correlation sequence of the~signal with itself.\\
            \\
            $[Rxx, lag] = xcorr(x, maxlag);$\\
            \\
            where $x$ is the~input signal and $maxlag$ is the~maximum lag to compute. $Rxx$ is the~cross-correlation
            sequence, and lag is the~corresponding lag vector.
            \item Find the~index of the~maximum value in the~autocorrelation function. We can use the~max function to
            find the~maximum value and its index.\\
            \\
            $[val, idx] = max(Rxx);$\\
            \\
            $val$ is ignored value of the~maximum, and idx is the~index of the~maximum value in the~Rxx vector.
            \item Calculate the~shift value from the~lag index. the~shift value is simply the~lag value
            corresponding to the~maximum value in the~autocorrelation function.\\
            \\
            $shift = lag(idx);$\\
            \\
           The~shift value is the~lag at which the~autocorrelation function has its maximum value, which
            corresponds to the~shift value for long-term linear prediction. Note that the~shift value is expressed in
            samples, so we may need to convert it to a~time delay if our signal has a~specific sampling rate.
        \end{enumerate}
        \newpage
        \subsection{Neural network approach} \label{subsec:neural}
        \textbf{Create neural network for shift and long shift estimation}\\
        \\
        To create a~neural network for shift and long shift prediction, we can follow these steps:
        \begin{enumerate}
            \item Define the~problem: the~first step is to define the~problem we want to solve. In this case, the
            problem is to predict the~shift and long shift of a~time series data.
            \item Collect and preprocess the~data: Collect the~data that we want to use for training and
            testing the~neural network. Preprocess the~data by normalizing it and splitting it into training,
            validation, and testing sets.
            \item Choose the~architecture: Choose the~architecture for our neural network based on the~problem
            we want to solve. In this case, a~recurrent neural network (RNN) architecture such as a~Long Short-Term
            Memory (LSTM) or Gated Recurrent Unit (GRU) may be suitable, as they are specifically designed to handle
            sequential data.
            \item Define the~input and output: Define the~input and output of our neural network. the~input should
            be the~time series data, and the~output should be the~predicted shift and long shift.
            \item Define the~loss function: Choose~an~appropriate loss function to measure the~difference
            between the~predicted shift and long shift and the~actual shift and long shift.
            \item Train the~model: Train the~model using the~training data and tune the~hyperparameters to
            improve the~performance of the~model. Monitor the~model's performance using the~validation set.
            \item Evaluate the~model: Evaluate the~performance of the~model on the~testing set to see how well
            it generalizes to new data.
            \item Fine-tune the~model: If the~model does not perform well on the~testing set, fine-tune the~model by
            adjusting the~architecture, hyperparameters, or training data.
            \item Use the~model: Once we have a~well-performing model, we can use it to predict the
            shift and long shift of new time series data.
        \end{enumerate}
        By following these steps, we can create a~neural network for shift and long shift prediction.
        It's important to experiment with different architectures and hyperparameters to find the~best
        model for our data.\\
        \\
        \textbf{Choose activation function}\\
        \\
        Choosing the~activation function for a~neural network for shift and long shift prediction depends on the
        architecture of the~network and the~problem being solved.\\
        \\
        For a~recurrent neural network (RNN) architecture such as Long Short-Term Memory (LSTM) or
        Gated Recurrent Unit (GRU), the~most commonly used activation function is the~hyperbolic tangent (tanh) function.
        This is because the~tanh function produces outputs in the~range $[-1, 1]$, which makes it suitable for RNNs
        that use recurrent connections to store and update memory over time.\\
        \\
        However, other activation functions such as the~Rectified Linear Unit (ReLU) or its variants, such as leaky
        ReLU or exponential ReLU, can also be used in RNNs for shift and long shift prediction. These activation
        functions are computationally efficient and can be used in deep neural networks without the~problem of
        vanishing gradients.\\
        \\
        In general, the~choice of activation function should be based on the~characteristics of the~problem being
        solved, the~architecture of the~network, and the~properties of the~activation function itself.
        Experimentation with different activation functions can help to determine the~most appropriate one
        for a~given problem.\\
        \\
        \textbf{Train neural network}\\
        \\
        To train our neural network, we utilized a training dataset derived from our main dataset consisting of six months of order data. The neural network was fed this dataset in order to adjust the weights of its connections between neurons and minimize the difference between the network's output and the correct output for each input, a process commonly referred to as backpropagation.\\
        \\
        It's important to experiment with different architectures and hyperparameters to find the~best model for
        our data. Additionally, it's important to properly preprocess the~data and monitor the~model's performance
        to avoid overfitting. With these steps, we can train a~neural network for shift and long shift prediction.\\
        \\
        \textbf{Run neural network to get optimal shift and long shift}\\
        Our trained Neural network is consist of 4 layers with 40 neurons in input layer than two hidden layers with 17 and 8 neurons and 6 neuron in output layer. Our trained network use leaky ReLU activation function. Training was made over 120 of observations.\\
        \\
        To run a~trained neural network for shift and long shift prediction, we can follow these steps:
        \begin{enumerate}
            \item Preprocess the~data: If we have new data that we want to use for prediction, preprocess it in the
            same way as the~training data by normalizing it and formatting it into sequences.
            \item Load the~trained model: Load the~trained model into memory using a~deep learning framework such as
            Tensorflow, PyTorch, or Keras.
            \item Prepare the~input data: Prepare the~input data by formatting it into sequences that match the~input
            shape of the~model.
            \item Make predictions: Use the~predict function of the~model to make predictions on the~input data.
            the~output of the~model will be the~predicted shift and long shift values.
            \item Postprocess the~output: If necessary, postprocess the~output of the~model by denormalizing it or
            transforming it back into the~original format.
            \item Evaluate the~predictions: Evaluate the~quality of the~predicted shift and long shift values by
            comparing them with the~actual shift and long shift values. Calculate performance metrics such as mean
            squared error or mean absolute error to quantify the~accurate of the~predictions.
            \item Refine the~model: If the~predicted shift and long shift values are not accurate enough,
            refine the~model by adjusting the~architecture, hyperparameters, or training data and repeat the~process.
        \end{enumerate}
        With these steps, we can run a~trained neural network for shift and long shift prediction and obtain optimal
        shift and long shift values. It's important to note that the~quality of the~predictions depends on the
        quality and quantity of the~data used for training and the~architecture and hyperparameters of the~model.
        Therefore, it's important to experiment with different models and training data to find the~optimal solution.
    \section{Estimation of seasonal weights}\label{sec:weights}
    To create a~mathematical model for predicting sales data with periodical trends, we can use a~seasonal
    ARIMA (SARIMA) model. This model takes into account seasonal variations in the~data and uses autoregressive and
    moving average terms to capture the~patterns and trends in the~data.\\
    \\
    To create the~weights for the~SARIMA model, we can follow these steps:
    \begin{enumerate}
        \item Identify the~seasonal period: Determine the~seasonal period of the~sales data. This can be done by
        analyzing the~autocorrelation function (ACF) and partial autocorrelation function (PACF) of the~data.
        \item Estimate the~parameters: Estimate the~parameters of the~SARIMA model using the~sales data.
        This can be done using statistical software such as R or Python.
        \item Interpret the~weights: Once the~parameters have been estimated, interpret the~weights of the~model
        to understand how they contribute to the~prediction. the~weights represent the~strength of the~relationship
        between the~sales data and the~predictor variables.
        \item Validate the~model: Validate the~model by comparing the~predicted sales data with the~actual sales data.
        Use statistical measures such as mean squared error or mean absolute error to evaluate the
        accurate of the~predictions.
        \item Refine the~model: Refine the~model by adjusting the~weights or the~parameters of the~model based on
        the~validation results. Repeat the~validation process until the~model provides accurate predictions.
    \end{enumerate}
    It's important to note that creating a~SARIMA model can be complex and requires knowledge of time series
    analysis and statistical modeling. Therefore, it's recommended to seek the~help of a~data scientist
    or a~statistician for assistance.

    \section{Extended long-term prediction}\label{sec:extlonglp}
    For my purpose i created extended long-term prediction which handle the~seasonality and repeated patterns
    in economics data. This correction mechanism is used to reflect historical peaks in graphs thrue dataset.
    This simple correction increase accurate of the~model and get better response due tue psychology,
    sociology and marketing aspects in dataset.\\
    \\
    To set up periodical weights its needs to create a~vector of correction parameters from original dataset using
    median and standard deviation statistical parameters from dataset. As a~base equation we will user equation
    for long-term prediction\ref{eq:ltlp} with the~new weights and get:
    \begin{equation} \label{eq:eltlp}
        \hat{x}(n) = \left(\sum_{i=1}^{p} a_i x(n-i) + \sum_{i=1}^{q} b_i x(n-S-i)\right) * \gamma(n),
    \end{equation}
    \\
    where $\hat{x}(n)$ is the~predicted value of the~order at time $n$, $x(n-i)$ are the~past short-therm predition part $p$ samples of the~dataset, $x(n-S-i)$ are the~past long-term prediction part with seasonal shift $S$, and $a_i$ and $b_i$ are the~predictors coefficients. The~order of the~predictor is $p$ for short-term and $q$ for the long-term linear prediction. The seasonal weights is represent by $\gamma(n)$.\\
    \\
    When building a~long-term prediction model, the~use of weights in this use case have a~significant effect on
    the~model's performance. Weights are used to assign different levels of importance to different
    features or inputs in the~model.\\
    \\
    In a~long-term prediction model, the~weight assigned to each input can determine its impact on the~model's predictions.
    Using weights in order prediction model, the~weight assigned to a~historical company's financial data are more important
    than the~weight assigned to its historical visitors. This is why we are ignore this data for prediction.\\
    \\
    If the~weights are not carefully chosen, the~model may give too much importance to some inputs and not enough to others,
    leading to inaccurate predictions. Therefore, selecting the~right weights is crucial for creating~an~accurate and
    effective long-term prediction model. In our case it reduce the~prediction error and returns more usable
    data for order prediction.\\
    \\
    In addition, the~choice of weights can also affect the~model's ability to adapt to changing conditions over time.
    a~model with carefully chosen weights can be more resilient and adaptable to changes in the~input data,
    while a~model with poorly chosen weights may be less flexible and struggle to adjust to new information.
    So it's really important to set correct shift and period to recalculation of the~shift.
    This is ride by neural network from \ref{subsec:neural}.\\
    \\
    Overall, the~use of weights is~an~important consideration in building our predictive model, and careful selection
    can significantly improve the~accurate and effectiveness of our long-term prediction model.

    \section{Novel approach to forecast sales data}\label{subsec:combining_models}
    In this chapter we prepare sub-parts of our novel aproache to forecast orders in sales companies.
    Here are the steps how to made our approache function and return accurate results for number of orders.
    \begin{enumerate}
        \item Run neural network to get right order of linear predictor. In this section we have trained Neural network from section~\ref{subsec:neural} and detect correct order from our neural network really fast.
        \item Run neural network to get righ shift and long shift of linear prediction.
        Than ve can run our next neural network trained for detect shift for our prediction. In this neural network as one of the~inputs we are using results from previous neural network.
        \item Run extended long-term prediction to ger predicted values of sales data.
        When we have optimal order and shift we are able to run linear prediction in a~standard way.
        \item Apply periodical wages to previous response data.
        Results from previous step are corrected by the~weights vector which is calculated over the~trained dataset.
        \item Calculate comparison criteria for results and plot data and mean square errors.
        When we run all the~models we are able to calculate comparison criteria from~\ref{subsec:comparison} and
        get a~table of results to plot errors and compare predicted and measured orders from validation dataset.
    \end{enumerate}
